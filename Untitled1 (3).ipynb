{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68716c6b-867c-4ac5-8fee-467e1fa693a6",
   "metadata": {},
   "source": [
    "Q1\n",
    "Overfitting occurs when a machine learning model learns the training data too well, capturing noise or random fluctuations in the data rather than the underlying patterns. This leads to poor generalization to new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying structure of the data, resulting in high bias and poor performance on both the training and test data.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "Overfitting: High variance, poor generalization to new data, and low performance on test data.\n",
    "Underfitting: High bias, inability to capture the underlying patterns in the data, and low performance on both training and test data.\n",
    "Mitigation:\n",
    "\n",
    "Overfitting: Use techniques like regularization, cross-validation, early stopping, and reducing model complexity.\n",
    "Underfitting: Increase model complexity, add more features, or use more advanced algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace279b-2486-491b-834a-40f6fc1384cb",
   "metadata": {},
   "source": [
    "Q2\n",
    "To reduce overfitting, one can:\n",
    "\n",
    "Use regularization techniques like L1 or L2 regularization.\n",
    "Cross-validation to assess model performance on different subsets of the data.\n",
    "Feature selection to remove irrelevant or redundant features.\n",
    "Early stopping during model training to prevent overfitting on the training data.\n",
    "Reduce model complexity by simplifying the architecture or using techniques like dropout in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b1d12-f1b2-4c96-9d88-2d27db0e7ad2",
   "metadata": {},
   "source": [
    "Q3\n",
    "Underfitting occurs when a model is too simple to capture the underlying structure of the data. It can happen in scenarios where:\n",
    "\n",
    "The model is too basic compared to the complexity of the data.\n",
    "Insufficient training data is provided.\n",
    "The features do not adequately represent the relationships in the data.\n",
    "The model's parameters are not properly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973eee14-a5e6-40f5-b275-e5437170c701",
   "metadata": {},
   "source": [
    "Q4\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the balance between bias and variance in model performance. Bias measures how closely the predictions of a model match the true values, while variance measures how much the predictions vary for different training sets.\n",
    "\n",
    "High bias models have simplified assumptions and may underfit the data.\n",
    "High variance models are overly complex and may overfit the data.\n",
    "The relationship between bias and variance is inversely proportional; as one decreases, the other increases.\n",
    "Balancing bias and variance is crucial for achieving optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7b681-3dac-462a-a241-ec88b0fcf9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
